---
title: "Exercise 5 - Zhenyun Shi, Huiwen, River"
date: "2024-03-11"
output: html_document
---
#Packages
```{r, message=F}
library(tidyverse) # loads dplyr, ggplot2, and others
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(topicmodels) # to estimate topic models
library(gutenbergr) # to get text data
library(scales)
library(tm)
library(ggthemes) # to make your plots look nice
library(readr)
library(quanteda)
library(quanteda.textmodels)
library(preText)
```

# 1.Choose another book or set of books from Project Gutenberg.

#I chose two books: "women" (21840) and "what a young women ought to know" (28458), and also extract the information of "author"
```{r}
tocq <- gutenberg_download(c(21840,28458),
                           meta_fields = "author")
```

#now I tokenize the two books, firstly we should categorize two books by creaing a colomn "booknumber", DiA1 for book ID 21840, DiA2 for book ID28458; secondly,I use "unnest_tokens (word, text)" to tokenize texts of tocq (two books) by words, also exclude those invalid words (n.a.), and count how many times each unique word occurs in each book. Finally, we should remove those typical stopwords.
```{r}
tocq_words <- tocq %>%
  mutate(booknumber = ifelse(gutenberg_id==21840, "DiA1", "DiA2")) %>%
  unnest_tokens(word, text) %>%
  filter(!is.na(word)) %>%
  count(booknumber, word, sort = TRUE) %>%
  ungroup() %>%
  anti_join(stop_words)
```

#now we transform the dataframe "tocq_words" to a document-terms-matrix "tocq_dtm", by using cast_dtm (booknumber, word, n). booknumber is the document, word=terms, n is the occurence of each word in this document.
```{r}
tocq_dtm <- tocq_words %>%
  cast_dtm(booknumber, word, n)

tm::inspect(tocq_dtm)
```
#we get the matrix, sparsity= 35% means only 35% of terms of the book will not appear in this matrix.

#now we turn to estimate our topic model and visualize it.

#1. set our seed that could make sure we can reproduce the same results each time. *also change the k value from 10 to 20

```{r}
tocq_lda <- LDA(tocq_dtm, k = 20, control = list(seed = 1234))
```

#now we can see the per-topic-per-word-probabilities (i.e., the beta of each unique word to this given document)

```{r}
tocq_topics <- tidy(tocq_lda, matrix = "beta")

head(tocq_topics, n = 20)
```
#we can see that "women" is the most often word and it most likely to belong to topic 18, now we can visualize by making a plot for the topi-per-term-per-row. Because of limited space, I only choose top 6 relevant word for each topic (total:20)

```{r}
tocq_top_terms <- tocq_topics %>% #create a varaible tocq_top_terms by extracting content from tocq_topics
  group_by(topic) %>% #now we group each terms by topic
  top_n(6, beta) %>% # we select only top 6 terms that has the highest data value. 
  ungroup() %>% #we ungroup this dataframe tocq_topics
  arrange(topic, -beta) #rearrange this variable by grouping terms by topic and sort them by their data value in desending order.

tocq_top_terms %>% #use the pipe symbol
  mutate(term = reorder_within(term, beta, topic)) %>% #we rectify the "term" column with the order "term, bata, topic" for the next visualization
  ggplot(aes(beta, term, fill = factor(topic))) + #we set the x axis as "beta", y axis as "term", and fill them colours based on each topic.
  geom_col(show.legend = FALSE) + #create a column plot, without legends.
  facet_wrap(~ topic, scales = "free", ncol = 4) + #sort them based on different topics, wihout sclaes, and each line have 4 topics.
  scale_y_reordered() + #recorder the y scale to be clear
  theme_tufte(base_family = "Helvetica") + #set the style of our plot and use the font "Helvetica"
  theme(axis.text.y = element_text(size = rel(0.7))) #reducing the size of font

```
######EVALUATING TOPIC MODEL

#1. plot relative word frequencies
```{r}
tidy_tocq <- tocq %>% #create dataframe "tidy_tocq" to contain the variables of tokenised data from "tocq"
  unnest_tokens (word, text) %>%
  anti_join(stop_words)
```

##count most common words in both
```{r}
tidy_tocq %>%
  count(word, sort = TRUE) #so count the frequency of each unique word in "tidy_tocq", and sort those word in descending order that means the most frequent word will be on the top 
```

```{r}
bookfreq <- tidy_tocq %>% #now we have the word frequency in the whole dataframe, but we need to know the frequency of each word in given particular "book"
  mutate(booknumber = ifelse(gutenberg_id==21840, "DiA1", "DiA2")) %>% # we assign the "DiA1" and "DiA2" to each book, 21840 and 28458 respectively.
  mutate(word = str_extract(word, "[a-z]+")) %>% # we only keep the word that is alphabetical (lowercase)
  count(booknumber, word) %>% #count the frequency of everyword to each book
  group_by(booknumber) %>% #and we group these values of n and words by specific booknumber
  mutate(proportion = n / sum(n))%>% #calculate the proportion of the word frequency in the total word of a book
  select(-n) %>% #delete the column "n"
  spread(booknumber, proportion) #only keep the column "booknumber" and "proportion"

ggplot(bookfreq, aes(x = DiA1, y = DiA2, color = abs (DiA1 - DiA2))) + # we set the "x" and "y" axis and also the color should represent the absolute difference of DiA1 and DiA2
  geom_abline(color = "black", lty = 2) + # we set the straight color and type of the line
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + # the value of alpha decide the transparency of points, from 0 to 1, 0.1 means very transparent, it might be because of large volume of words.
  geom_text(aes (label = word), check_overlap = TRUE, vjust = 1.5) + #each point should be labeled with given word, and check the overlapping points to keep distance, and also the vertical distance between labels and points should be 1.5
  scale_x_log10(labels = percent_format()) + #since we are dealing with proprotion values, we should log these value 
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c (0, 0.001), low = "pink", high = "pink3")+ # we make sure each mall point is distinguishable, and define the colors as "pink" for low and darkpink for high
  theme_tufte(base_family = "Helvetica") +
  theme(legend.position="none", 
        strip.background = element_blank(), 
        strip.text.x = element_blank()) +
  labs(x = "Women", y = "What a young women ought to know") +
  coord_equal() #set the aspect ratio of the plot such that one unit on the x-axis is equal to one unit on the y-axis
  
```
# this plot means that those more physical concepts appear with greater frequency in Volume 2, while more abstract words of women in Volume 1.

```{r}
# Filtering out missing text passages, labeling book chapters, decomposing text into words, and calculating the frequency of occurrence of words other than stop words in each chapter.

tocq <- tocq %>%
  filter(!is.na(text))

tocq_chapter <- tocq %>%
  mutate(booknumber = ifelse(gutenberg_id==21840, "Book1", "Book2")) %>%
  group_by(booknumber) %>%
  mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>% # Calculate the chapter number by detecting the beginning of the chapter.
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, booknumber, chapter) # Combine book number and chapter number into a new column `document`.

tocq_chapter_word <- tocq_chapter %>%
  unnest_tokens(word, text)

tocq_word_counts <- tocq_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()
```

```{r}
# Create a document-term-matrix.

tocq_word_counts

tocq_chapters_dtm <- tocq_word_counts %>%
  cast_dtm(document, word, n)

tm::inspect(tocq_chapters_dtm)
```

```{r}
tocq_chapters_lda <- LDA(tocq_chapters_dtm, k = 2, control = list(seed = 1234))

tocq_chapters_gamma <- tidy(tocq_chapters_lda, matrix = "gamma")
tocq_chapters_gamma
```
Examine consensus

```{r}
# Identify and compare the most relevant topics across chapters and throughout the book.

tocq_chapters_gamma <- tocq_chapters_gamma %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

tocq_chapter_classifications <- tocq_chapters_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>% # Select the highest gamma value (the most relevant topic) in each chapter.
  ungroup()

tocq_book_topics <- tocq_chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  top_n(1, n) %>% # Selection of the most frequently occurring themes.
  ungroup() %>%
  transmute(consensus = title, topic) # Conversion results in renaming the book listings to `consensus`.

tocq_chapter_classifications %>%
  inner_join(tocq_book_topics, by = "topic") %>%
  filter(title != consensus)
```

```{r}
# The results of the model are combined with the raw data.

assignments <- augment(tocq_chapters_lda, data = tocq_chapters_dtm)
assignments
```

```{r}
assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>% # Split document identifiers into book titles and chapter numbers.
  inner_join(tocq_book_topics, by = c(".topic" = "topic")) # Intralink chapter topic assignment information to the major themes of each book

assignments %>% # Set up the charts and do the same below.
  count(title, consensus, wt = count) %>%
  group_by(title) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, title, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  geom_text(aes(x = consensus, y = title, label = scales::percent(percent))) +
  theme_tufte(base_family = "Helvetica") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Book words assigned to",
       y = "Book words came from",
       fill = "% of assignments")
```

# 3.Validate different pre-processing techniques using preText on the new book(s) of your choice.

```{r}
# Randomly select a sample of 1000 documents as a smaller subset to analyze and print out the names of the top 10 documents.

corp <- corpus(tocq, text_field = "text") # Create a text corpus, using the "text" column in the tocq data frame as the text source.
documents <- corp[sample(1:10000,1000)] # 1000 documents were randomly selected from the corpus.
print(names(documents[1:10])) # Prints the names of the first 10 selected documents.
```

```{r}
preprocessed_documents <- factorial_preprocessing(
    documents,
    use_ngrams = TRUE,
    infrequent_term_threshold = 0.2,
    verbose = FALSE)
```

```{r}
preText_results <- preText(
    preprocessed_documents,
    dataset_name = "Tocqueville text",
    distance_method = "cosine",
    num_comparisons = 20,
    verbose = FALSE)
```

```{r}
preText_score_plot(preText_results)
```
